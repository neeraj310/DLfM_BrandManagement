{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Retrieval From Instagram\n",
    "\n",
    "**Goal:** collect image data from instagram and then preprocess it, extract information (image files) from a user's Instagram profile\n",
    "\n",
    "**Constraints:** the user has no way of setting the image size (in KB), the resolution (1080x1080) of the images found on Instagram. The images are extracted from the Instagram page in raw form.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Websites: \n",
    "\n",
    "This notebook's code is based on the following tutorials: \n",
    "\n",
    "https://medium.com/@srujana.rao2/scraping-instagram-with-python-using-selenium-and-beautiful-soup-8b72c186a058\n",
    "\n",
    "https://edmundmartin.com/scraping-instagram-with-python/\n",
    "\n",
    "https://michaeljsanders.com/2017/05/12/scrapin-and-scrollin.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** *Remember to respect user’s rights when you download copyrighted content. Do not use images/videos from Instagram for commercial intent.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies\n",
    "\n",
    "Install non-standard libraries: requests, BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import choice\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import shutil\n",
    "# to install\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build InstagramScraper class\n",
    "based on: https://edmundmartin.com/scraping-instagram-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching user agents is often a best practice when web scraping and can help you avoid detection. Should the caller of our class have provided their own list of user agents we take a random agent from the provided list.  Otherwise we will return our default user agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class called InstagramScraper: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url header for requests.get()\n",
    "headers={'User-Agent':  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "         ,  'content-type': 'application/json'\n",
    "         , 'accept-encoding': 'gzip, deflate, br'\n",
    "         , 'cache-control': 'no-cache'\n",
    "         , 'accept' : '*/*'\n",
    "         , 'accept-language' : 'de-DE, de; q=0.9,en-US; q=0.8,en;q=0.7'\n",
    "         #, 'referer' : url\n",
    "         , 'connection' : 'keep-alive'\n",
    "         , 'cookie' : 'ig_cb=1; ig_did=DA66C494-9DFE-48F6-BA63-66F11DF8EC03; csrftoken=ukE8jYSjQxVs1YGPYddEkAXsN6WZ4Qmw; mid=XoChrAALAAG78Upva7Ld0TAzeTtm; rur=ASH; urlgen=\"{\\\"2a04:ee41:4:95:91f9:b9d4:8aab:41c\\\": 15796\\054 \\\"213.55.241.7\\\": 15796\\054 \\\"2a04:ee41:4:95:60ae:def3:2fd7:3633\\\": 15796}:1jIpww:PTjjrSzpjC6dWww8-AVOnfdQAFA\"'\n",
    "        }\n",
    "_user_agents = [\n",
    "   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstagramScraper:\n",
    "\n",
    "    def __init__(self, user_agents=None, proxy=None):\n",
    "        self.user_agents = user_agents\n",
    "        self.proxy = proxy\n",
    "\n",
    "    def __random_agent(self):\n",
    "        if self.user_agents and isinstance(self.user_agents, list):\n",
    "            return choice(self.user_agents)\n",
    "        return choice(_user_agents)\n",
    "\n",
    "    def __request_url(self, url):\n",
    "        \"\"\"Our second helper method is simply a wrapper around requests. \n",
    "        We pass in a URL and try to make a request using the provided user agent and proxy. \n",
    "        If we are unable to make the request or Instagram responds with a non-200 status code we simply re-raise the error. \n",
    "        If everything goes fine, we return the page in questions HTML.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers={'User-Agent': self.__random_agent()}, proxies={'http': self.proxy, 'https': self.proxy})\n",
    "            #response = requests.get(url, headers=headers, proxies={'http': self.proxy, 'https': self.proxy})\n",
    "            response.raise_for_status()\n",
    "        except requests.HTTPError:\n",
    "            raise requests.HTTPError('Received non 200 status code from Instagram')\n",
    "        except requests.RequestException:\n",
    "            raise requests.RequestException('Internet connection failed.')\n",
    "        else:\n",
    "            return response.text\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_json_data(html):\n",
    "        \"\"\"Instagram serve’s all the of information regarding a user in the form of JavaScript object. \n",
    "        This means that we can extract all of a users profile information and their recent posts by just \n",
    "        making a HTML request to their profile page. We simply need to turn this JavaScript object into \n",
    "        JSON, which is very easy to do.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        body = soup.find('body')\n",
    "        script_tag = body.find('script')\n",
    "        #pprint(script_tag)\n",
    "        #print('\\n')\n",
    "        #pprint(type(script_tag))\n",
    "        content = script_tag.contents#.strip().replace('window._sharedData =', '').replace(';', '')\n",
    "        #######\n",
    "        content_string = ''.join(content)\n",
    "        raw_string = content_string.strip().replace('window._sharedData =', '').replace(';', '')\n",
    "        #######\n",
    "        #print('\\n')\n",
    "        #pprint(raw_string)\n",
    "        #print('\\n')\n",
    "        #pprint(type(raw_string))\n",
    "        return json.loads(raw_string)\n",
    "\n",
    "    def profile_page_metrics(self, profile_url):\n",
    "        results = {}\n",
    "        try:\n",
    "            response = self.__request_url(profile_url)\n",
    "            json_data = self.extract_json_data(response)\n",
    "            metrics = json_data['entry_data']['ProfilePage'][0]['graphql']['user']\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        else:\n",
    "            for key, value in metrics.items():\n",
    "                #print('key:', key, '-value:', value)\n",
    "                if key != 'edge_owner_to_timeline_media':\n",
    "                    if value and isinstance(value, dict):\n",
    "                        value = value['count']\n",
    "                        results[key] = value\n",
    "                    elif value:\n",
    "                        results[key] = value\n",
    "        return results\n",
    "\n",
    "    \n",
    "    def hash_page_metrics(self, profile_url):\n",
    "        results = {}\n",
    "        try:\n",
    "            response = self.__request_url(profile_url)\n",
    "            json_data = self.extract_json_data(response)\n",
    "            metrics = json_data['entry_data']['TagPage'][0]['graphql']['hashtag']\n",
    "         \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        else:\n",
    "            for key, value in metrics.items():\n",
    "                #print('metrics:', metrics)\n",
    "                if key != 'edge_hashtag_to_media' and key != 'edge_hashtag_to_top_posts' and key != 'profile_pic_url':\n",
    "                    results[key] = value\n",
    "                    if value and isinstance(value, dict):\n",
    "                        try: \n",
    "                            value = value['count']            \n",
    "                            results[key] = value\n",
    "                        except: \n",
    "                            results[key] = value\n",
    "                        try: \n",
    "                            sigma = []\n",
    "                            for i in range(0,5): \n",
    "                                #print(i)\n",
    "                                value = value['edges'][i]['node']['name']  \n",
    "                                #print(i)\n",
    "                            sigma.append(value)\n",
    "                            print(len(value['edges']['node']))\n",
    "                            \n",
    "                            #results[key] = sigma\n",
    "                        except: \n",
    "                            results[key] = value \n",
    "                    elif value:\n",
    "                        results[key] = value\n",
    "        return results\n",
    "    \n",
    "    def profile_page_posts(self, profile_url):\n",
    "        results = []\n",
    "        try:\n",
    "            response = self.__request_url(profile_url)\n",
    "            #pprint(response)\n",
    "            json_data = self.extract_json_data(response)\n",
    "            #pprint(json_data)\n",
    "            metrics = json_data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media'][\"edges\"]\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        else:\n",
    "            for node in metrics:\n",
    "                node = node.get('node')\n",
    "                #if node and isinstance(node, dict): #this line only gets most recent post out\n",
    "                results.append(node)\n",
    "        return results\n",
    "    \n",
    "    def hashtag_page_posts(self, hashtag_url):\n",
    "        results = []\n",
    "        try:\n",
    "            response = self.__request_url(hashtag_url)\n",
    "            json_data = self.extract_json_data(response)\n",
    "            #pprint(json_data)\n",
    "            metrics = json_data['entry_data']['TagPage'][0]['graphql']['hashtag']['edge_hashtag_to_media'][\"edges\"]\n",
    "            #pprint(metrics)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        else:\n",
    "            for node in metrics:\n",
    "                node = node.get('node')\n",
    "                #if node and isinstance(node, dict): #this line only gets most recent post out\n",
    "                results.append(node)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load URLS of Brand Names Data\n",
    "\n",
    "This chapter is only applicable if you have a csv-file with a limited set of brands you want to scrape. If you do not want to limit yourself to a predefined list, you can skip this part. Specify instragram USERNAME profile whose page you want to scrape. Get a dictionary with all information (image, comments, etc.) from that Instagram profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to specify\n",
    "try: \n",
    "    directory= r'C:\\Users\\Anonym\\Documents\\GitHub\\DLfM_BrandManagement\\data\\instagram_urls'\n",
    "    os.chdir(directory)\n",
    "except: \n",
    "    directory = r'C:\\Users\\lsamsi\\Documents\\GitHub\\DLfM_BrandManagement\\data\\instagram_urls'\n",
    "    os.chdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get out all apparel brands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "\n",
    "#data = pd.read_csv(\"firm_usernames.csv\", header=None)\n",
    "\n",
    "#firm_usernames = data[0].tolist()\n",
    "#firm_usernames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"instagram_hashtags.csv\", header=None)\n",
    "\n",
    "#instagram_hashtags = data[0].tolist()\n",
    "#instagram_hashtags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform set theory on both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def intersection(lst1, lst2): \n",
    " #   lst3 = [value for value in lst1 if value in lst2] \n",
    "  #  return lst3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def non_intersection(lst1, lst2): \n",
    " #   lst3 = list(set(lst1) ^ set(lst2))\n",
    "  #  return lst3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def set_difference(lst1, lst2): \n",
    " #   lst3 = list(set(lst1) - set(lst2))\n",
    "  #  return lst3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Number of brands firm usernames: ', len(firm_usernames))\n",
    "#print('Number of brands as instagram hashtags: ', len(instagram_hashtags))\n",
    "#print('Number of same brands (firm usernames and hashtags): ', len(intersection(firm_usernames, instagram_hashtags)))\n",
    "#print('Brands that are both firm usernames and hashtags: ', intersection(firm_usernames, instagram_hashtags)[:5], '...')\n",
    "#print('Brands that are in neither firm usernames nor hashtags: ', non_intersection(firm_usernames, instagram_hashtags)[:5])\n",
    "#print('Brands that are firm usernames only: ', set_difference(firm_usernames, instagram_hashtags)[:5])\n",
    "#print('Brands that are hashtagged only: ', set_difference(instagram_hashtags, firm_usernames)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Specify Instagram page(s)\n",
    "\n",
    "Specify instragram USERNAME profile whose page you want to scrape. Get a dictionary with all information (image, comments, etc.) from that Instagram profile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert unofficial hashtag to official user-profile name \n",
    "\n",
    "For 'cailler' the '#cailler' user-input will get results on Instagram. The official Instagram of cailler might differ, however. \n",
    "The official brandname on Instagram is 'cailler-suisse'. Thus, we need a dataframe to get out the corresponding official name given the unofficial name. \n",
    "\n",
    "This is the reason why **we can only have brands that are listed in this dataframe** and **no other brands**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory \n",
    "#import os\n",
    "#directory= r\"C:\\Users\\Anonym\\Documents\\GitHub\\DLfM_BrandManagement\\data\\instagram_urls\"\n",
    "#os.chdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe \n",
    "import pandas as pd \n",
    "\n",
    "convert = pd.read_csv('hashToOfficialName.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instagram_hashtag</th>\n",
       "      <th>firm_account</th>\n",
       "      <th>brand_full_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abercrombie</td>\n",
       "      <td>abercrombie</td>\n",
       "      <td>Abercrombie &amp; Fitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas</td>\n",
       "      <td>Adidas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anntaylor</td>\n",
       "      <td>anntaylor</td>\n",
       "      <td>Ann Taylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bacardi</td>\n",
       "      <td>bacardiusa</td>\n",
       "      <td>Bacardi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bananarepublic</td>\n",
       "      <td>bananarepublic</td>\n",
       "      <td>Banana Republic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instagram_hashtag    firm_account       brand_full_name\n",
       "0       abercrombie     abercrombie   Abercrombie & Fitch\n",
       "1            adidas          adidas                Adidas\n",
       "2         anntaylor       anntaylor            Ann Taylor\n",
       "3           bacardi      bacardiusa               Bacardi\n",
       "4    bananarepublic  bananarepublic       Banana Republic"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pages that have access denial because of age limit\n",
    "# are you 18/21 or over? \n",
    "#urls.remove('https://www.instagram.com/bacardiusa/?hl=en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items to be removed from list\n",
    "#agelimited_brands = {'bacardiusa', 'budlight', 'budweiser', 'coorslight', 'corona', 'greygoose', 'jackdaniels_us', 'korbel_1882'} \n",
    "  \n",
    "#firm_usernames = [ele for ele in firm_usernames if ele not in agelimited_brands] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instagram_hashtag</th>\n",
       "      <th>firm_account</th>\n",
       "      <th>brand_full_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abercrombie</td>\n",
       "      <td>abercrombie</td>\n",
       "      <td>Abercrombie &amp; Fitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas</td>\n",
       "      <td>Adidas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anntaylor</td>\n",
       "      <td>anntaylor</td>\n",
       "      <td>Ann Taylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bananarepublic</td>\n",
       "      <td>bananarepublic</td>\n",
       "      <td>Banana Republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bigelow</td>\n",
       "      <td>bigelowtea</td>\n",
       "      <td>Bigelow`s Tea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instagram_hashtag    firm_account       brand_full_name\n",
       "0       abercrombie     abercrombie   Abercrombie & Fitch\n",
       "1            adidas          adidas                Adidas\n",
       "2         anntaylor       anntaylor            Ann Taylor\n",
       "4    bananarepublic  bananarepublic       Banana Republic\n",
       "5           bigelow      bigelowtea         Bigelow`s Tea"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# items to be removed from conversion dataframe \n",
    "agelimited_brands = ['bacardiusa', 'budlight', 'budweiser', 'coorslight', 'corona', 'greygoose', 'jackdaniels_us', 'korbel_1882'] \n",
    "\n",
    "convert = convert[~(convert.firm_account.isin(agelimited_brands))]\n",
    "convert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashToOfficial(hashing): \n",
    "    username = convert.loc[convert['instagram_hashtag'] == hashing, 'firm_account'].iloc[0]\n",
    "    return username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyword input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_on_display = convert['instagram_hashtag'].tolist()\n",
    "brands_on_display = ', '.join(brands_on_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose from these brandnames to get a brand management analysis: abercrombie, adidas, anntaylor, bananarepublic, bigelow, carhartt, cocacola, converse, dockers, dolcegabbana, domperignon, drpepper, eddiebauer, fanta, gap, gatorade, gucci, guess, hanes, hollister, honesttea, jcrew, joeboxer, juicycouture, kennethcole, levis, lipton, llbean, luckybrand, moetchandon, monsterenergy, nesquik, oldnavy, oshkosh, prada, ralphlauren, sanpellegrino, snapple, tazo, tommyhilfiger, underarmour, urbanoutfitters, victoriassecret, vitaminwater, welchs, minutemaid, motts, swissmiss\n",
      "Which brandname do you want to analyze?nestlé\n"
     ]
    }
   ],
   "source": [
    "print('Choose from these brandnames to get a brand management analysis:', brands_on_display)\n",
    "keyword = input('Which brandname do you want to analyze?')\n",
    "# 'sanpellegrino'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag Page\n",
    "\n",
    "If you want to open a hashtag page (instead of a user profile): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multiple brands  \n",
    "#hash_urls = []\n",
    "#username = False\n",
    "\n",
    "#for hashtag in instagram_hashtags: \n",
    " #   url = 'https://www.instagram.com/explore/tags/'+hashtag\n",
    "  #  hash_urls.append(url)\n",
    "\n",
    "#hash_urls[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one brand only \n",
    "\n",
    "# to specify user_input\n",
    "hashtag = 'nestlé' \n",
    "hash_url = 'https://www.instagram.com/explore/tags/'+hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-profile Page\n",
    "\n",
    "If you want to scrape a user-profile page, specify the username as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multiple firms  \n",
    "#urls = []\n",
    "#hashtag = False\n",
    "\n",
    "#for username in firm_usernames: \n",
    " #   url = 'https://www.instagram.com/'+username+'/?hl=en'\n",
    "  #  urls.append(url)\n",
    "\n",
    "#urls[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one firm only \n",
    "\n",
    "# to specify user_input\n",
    "username= \"nestle\"\n",
    "#username= 'swatch'\n",
    "url = 'https://www.instagram.com/'+username+'/?hl=en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get information from Instagram page(s) [optional]\n",
    "\n",
    "Now that the url of the Instagram page is defined, it will extract out all the posts or meta-information from the website usinge the InstagramScraper class. \n",
    "\n",
    "Get meta-information metrics by using a class method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get profile page metrics\n",
    "#from pprint import pprint\n",
    "\n",
    "#k = InstagramScraper()\n",
    "#results = k.profile_page_metrics(url) \n",
    "#pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hashtag page metrics\n",
    "#from pprint import pprint\n",
    "\n",
    "#k = InstagramScraper()\n",
    "#TODO\n",
    "#results = k.hash_page_metrics(url) \n",
    "#pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get image posts from Instagram page(s)\n",
    "\n",
    "Get all posts on an Instagram **profile page** that are visible on the landing page (more items only load as you scroll downwards). The page loads 12 items at a time, and I need to scroll to load all entries (for a total of 120)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-profile Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get posts (images) from multiple profile pages \n",
    "#from pprint import pprint\n",
    "\n",
    "#resultz = []\n",
    "#for url in urls: \n",
    " #   k = InstagramScraper()\n",
    "  #  results = k.profile_page_posts(url)\n",
    "   # resultz.append(results)\n",
    "    #print('Instagram page: ', url)\n",
    "\n",
    "#print('Total number of Instagram user-profile pages: ', len(resultz))\n",
    "#print('Total number of images: ', len(resultz)*len(resultz[0]))\n",
    "#print('Average number of images per Instagram user-profile page: ', len(resultz)*len(resultz[0])/len(resultz) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instagram page:  https://www.instagram.com/nestle/?hl=en\n",
      "Posts on Instagram profile page:  12\n",
      "Second image url on instagram profile:  https://instagram.fzrh2-1.fna.fbcdn.net/v/t51.2885-15/e35/92550584_846774565843081_8318181282322828356_n.jpg?_nc_ht=instagram.fzrh2-1.fna.fbcdn.net&_nc_cat=101&_nc_ohc=os27ce6mZuwAX9_eRH-&oh=5973906d249c2599b4ce462cf1e28468&oe=5E982618\n"
     ]
    }
   ],
   "source": [
    "# get posts (images) from single profile page \n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "k = InstagramScraper()\n",
    "results = k.profile_page_posts(url)\n",
    "\n",
    "print('Instagram page: ', url)\n",
    "print('Posts on Instagram profile page: ', len(results))\n",
    "print('Second image url on instagram profile: ', results[1]['display_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag Page\n",
    "\n",
    "Get all posts on an Instagram **hashtag page** that are visible on the landing page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get posts (images) from multiple hashtag pages \n",
    "#from pprint import pprint\n",
    "\n",
    "#hash_result = []\n",
    "#for url in hash_urls: \n",
    " #   k = InstagramScraper()\n",
    "  #  results = k.hashtag_page_posts(url)\n",
    "   # hash_result.append(results)\n",
    "    #print('Instagram page: ', url)\n",
    "\n",
    "#print('Total number of Instagram hashtag pages: ', len(hash_result))\n",
    "#print('Total number of hashed images: ', len(hash_result)*len(hash_result[0]))\n",
    "#print('Average number of images per Instagram hashtag page: ', len(hash_result)*len(hash_result[0])/len(hash_result) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get posts (images) from a hashtag page \n",
    "from pprint import pprint\n",
    "\n",
    "k = InstagramScraper()\n",
    "hash_results = k.hashtag_page_posts(hash_url)\n",
    "\n",
    "print('Instagram page: ', url)\n",
    "print('Posts on Instagram hashtag page: ', len(hash_results))\n",
    "print('Second image url on instagram hashtag: ', hash_results[1]['display_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save images into folders\n",
    "\n",
    "Save images from list of dict: Use requests library to download images from the ‘display_url’ in pandas ‘result’ data frame and store them with respective shortcode as file name.\n",
    "\n",
    "Specify the directory for storing the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import os\n",
    "import requests\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_root_path_images(): \n",
    "    \n",
    "    # to specify\n",
    "    try: \n",
    "        directory= r\"C:\\Users\\Anonym\\Documents\\GitHub\\DLfM_BrandManagement\\data\"\n",
    "        os.chdir(directory)\n",
    "    except: \n",
    "        directory= r\"C:\\Users\\lsamsi\\Documents\\GitHub\\DLfM_BrandManagement\\data\"\n",
    "        os.chdir(directory)\n",
    "    folder = 'instagram_images' #image root folder, all subfolders' name are firmnames\n",
    "\n",
    "    os.chdir(directory)\n",
    "\n",
    "    try: \n",
    "        os.mkdir(folder)\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    path = os.path.join(directory, folder)\n",
    "    os.chdir(path)\n",
    "    return path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_folders_images(account, folder, path): \n",
    "        try: \n",
    "            os.mkdir(os.path.join(path, account))\n",
    "        except: \n",
    "            pass\n",
    "        \n",
    "        # set directory \n",
    "        directory = os.path.join(path, account)\n",
    "        os.chdir(directory)   \n",
    "        try: \n",
    "            os.mkdir(folder)\n",
    "            print('new folder created for: ', account)\n",
    "        except: \n",
    "            pass\n",
    "        path = os.path.join(directory, folder)\n",
    "        os.chdir(path)\n",
    "        return path \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### User-profile page\n",
    " \n",
    " Save all images from user-profile Instagram pages to your folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all visible images from multiple Instagram pages \n",
    "\n",
    "#for i, username in enumerate(firm_usernames): \n",
    " #   path = set_root_path_images()\n",
    "  #  build_folders_images(username, 'user_profile', path)\n",
    "\n",
    "    # get image url \n",
    "   # for j in range(len(resultz[i])): \n",
    "    #    r = requests.get(resultz[i][j]['display_url'], stream=True)\n",
    "     #   with open(resultz[i][j]['shortcode']+\".jpg\", 'wb') as f:\n",
    "            # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "      #      r.raw.decode_content = True\n",
    "            # Copy the response stream raw data to local image file.\n",
    "       #     shutil.copyfileobj(r.raw, f)\n",
    "            # Remove the image url response object.\n",
    "        #    del r\n",
    "            \n",
    "   # print('processed: ', username, ' .', i, ' out of ', len(firm_usernames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Hashtag page\n",
    " \n",
    " Save all images from hashtag Instagram pages to your folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all visible images from multiple Instagram pages \n",
    "\n",
    "#for i, hashtag in enumerate(instagram_hashtags):\n",
    " #   path = set_root_path_images()\n",
    "  #  build_folders_images(hashtag, 'hashtag', path)\n",
    "\n",
    "    # get image url \n",
    "    #for j in range(len(hash_result[i])): \n",
    "     #   r = requests.get(hash_result[i][j]['display_url'], stream=True)\n",
    "      #  with open(hash_result[i][j]['shortcode']+\".jpg\", 'wb') as f:\n",
    "            # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "       #     r.raw.decode_content = True\n",
    "            # Copy the response stream raw data to local image file.\n",
    "        #    shutil.copyfileobj(r.raw, f)\n",
    "            # Remove the image url response object.\n",
    "         #   del r\n",
    "   # print('processed: ', hashtag, ' .', i, ' out of ', len(instagram_hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Both pages\n",
    " \n",
    " Save all images from both user profile and hashtag Instagram pages to your folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all visible images from multiple Instagram pages \n",
    "\n",
    "#if firm_usernames: \n",
    " #   for i, username in enumerate(firm_usernames): \n",
    "  #      path = set_root_path_images()\n",
    "   #     build_folders_images(username, 'user_profile', path)\n",
    "       \n",
    "        # get image url \n",
    "    #    for j in range(len(resultz[i])): \n",
    "     #       r = requests.get(resultz[i][j]['display_url'], stream=True)\n",
    "      #      with open(resultz[i][j]['shortcode']+\".jpg\", 'wb') as f:\n",
    "                # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "       #         r.raw.decode_content = True\n",
    "                # Copy the response stream raw data to local image file.\n",
    "        #        shutil.copyfileobj(r.raw, f)\n",
    "                # Remove the image url response object.\n",
    "         #       del r\n",
    "\n",
    "#elif instagram_hashtags: \n",
    " #   for i, hashtag in enumerate(instagram_hashtags):\n",
    "  #      path = set_root_path_images()\n",
    "   #     build_folders_images(hashtag, 'hashtag', path)\n",
    "\n",
    "        # get image url \n",
    "    #    for j in range(len(hash_result[i])): \n",
    "     #       r = requests.get(hash_result[i][j]['display_url'], stream=True)\n",
    "      #      with open(hash_result[i][j]['shortcode']+\".jpg\", 'wb') as f:\n",
    "                # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "       #         r.raw.decode_content = True\n",
    "                # Copy the response stream raw data to local image file.\n",
    "        #        shutil.copyfileobj(r.raw, f)\n",
    "                # Remove the image url response object.\n",
    "         #       del r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all visible images from an Instagram page \n",
    "\n",
    "#path = set_root_path_images()\n",
    "\n",
    "#if username: \n",
    "#    build_folders_images(username, 'user_profile', path)\n",
    "#elif hashtag: \n",
    " #   build_folders_images(hashtag, 'hashtag', path)\n",
    "\n",
    "#for i in range(len(results)):\n",
    " #   r = requests.get(results[i]['display_url'], stream=True)\n",
    "  #  with open(results[i]['shortcode']+\".jpg\", 'wb') as f:\n",
    "        # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "   #     r.raw.decode_content = True\n",
    "        # Copy the response stream raw data to local image file.\n",
    "    #    shutil.copyfileobj(r.raw, f)\n",
    "        # Remove the image url response object.\n",
    "     #   del r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download one image only\n",
    "\n",
    "#path = set_root_path_images()\n",
    "\n",
    "#r = requests.get(url, stream=True)\n",
    "\n",
    "#with open(directory+\"B-Tckr0AgrH\"+\".jpg\", 'wb') as f:\n",
    "    # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    " #   r.raw.decode_content = True\n",
    "    # Copy the response stream raw data to local image file.\n",
    "  #  shutil.copyfileobj(r.raw, f)\n",
    "    # Remove the image url response object.\n",
    "   # del r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save images\n",
    "\n",
    "Specify the directory for storing the images.\n",
    "\n",
    "Save images onto your PC, then load the images into a numpy array (variable) - for official and unofficial images of a brand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-profile page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load modules \n",
    "import imageio\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# set directory \n",
    "path = set_root_path_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new folder created for:  margotrobbie\n",
      "Directory set to:  C:\\Users\\Anonym\\Documents\\GitHub\\DLfM_BrandManagement\\data\\instagram_images\\margotrobbie\\official\n"
     ]
    }
   ],
   "source": [
    "# create folders\n",
    "build_folders_images(hashtag, 'official', path)\n",
    "print('Directory set to: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/abhaymise/b011f9d68456f1d87561d71af2f7fd6a\n",
    "\n",
    "# save images to PC \n",
    "for i in range(len(results)):\n",
    "    r = requests.get(results[i]['display_url'], stream=True)\n",
    "    with open(f\"{i}_\"+results[i]['shortcode']+\".png\", 'wb') as f:\n",
    "        # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "        r.raw.decode_content = True\n",
    "        # Copy the response stream raw data to local image file.\n",
    "        shutil.copyfileobj(r.raw, f)\n",
    "        # Remove the image url response object.\n",
    "        del r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify image dimension\n",
    "#IMG_WIDTH=300\n",
    "#IMG_HEIGHT=300\n",
    "#IMG_DIM = (IMG_WIDTH, IMG_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(300, 300, 3)\n",
      "(12, 300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# specify the size of the image\n",
    "target_size = (300,300)\n",
    "\n",
    "images_list = []\n",
    "for result in range(len(results)):\n",
    "    # get the image based on the 'display_url'\n",
    "    response = requests.get(results[result]['display_url'], stream=True)\n",
    "    # convert it into a bytes object\n",
    "    bytes = BytesIO(response.content)\n",
    "    # convert it into an Image object\n",
    "    image = Image.open(bytes)\n",
    "    \n",
    "    # resize the image if necessary\n",
    "    if image.size != target_size:\n",
    "        image = image.resize(target_size)\n",
    "    \n",
    "    # convert the image to a keras array and finally to a numpy array    \n",
    "    train_image = img_to_array(image)\n",
    "    train_image = np.array(train_image)\n",
    "    print(train_image.shape)\n",
    "    \n",
    "    images_list.append(train_image)\n",
    "    \n",
    "# convert the images_list into one numpy array (used as X_test for the model)\n",
    "images_np = np.stack(images_list, axis=0)\n",
    "print(images_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules \n",
    "#from io import BytesIO\n",
    "#import base64\n",
    "#from PIL import Image\n",
    "#from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# list of images as np.arrays \n",
    "#images_lst = []\n",
    "#np.array([0])\n",
    "#for i in range(len(results)):\n",
    "    # load image \n",
    "    #byteImg = Image.open(results[i]['shortcode']+\".png\")\n",
    "    # image into numpy array \n",
    "    #img_np = np.array(byteImg)\n",
    "    #images_np.append(img_np)\n",
    "    \n",
    "    # load image \n",
    "    #train_imgs = img_to_array(load_img(f\"{i}_\"+results[i]['shortcode']+\".png\", target_size=IMG_DIM))\n",
    "    #train_imgs = np.array(train_imgs)\n",
    "    #images_lst.append(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all images as numpy array (for feeding as X_test)\n",
    "#images_np = np.stack(images_lst, axis=0)\n",
    "#print(images_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory \n",
    "path = set_root_path_images()\n",
    "\n",
    "np.save(f'{hashtag}_official_npimgs.npy', images_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules \n",
    "import imageio\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# set directory \n",
    "path = set_root_path_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders\n",
    "build_folders_images(hashtag, 'unofficial', path)\n",
    "print('Directory set to: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/abhaymise/b011f9d68456f1d87561d71af2f7fd6a\n",
    "\n",
    "# save images to PC\n",
    "for i in range(len(hash_results)):\n",
    "    r = requests.get(hash_results[i]['display_url'], stream=True)\n",
    "    with open(f\"{i}_\"+hash_results[i]['shortcode']+\".png\", 'wb') as f:\n",
    "        # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "        r.raw.decode_content = True\n",
    "        # Copy the response stream raw data to local image file.\n",
    "        shutil.copyfileobj(r.raw, f)\n",
    "        # Remove the image url response object.\n",
    "        del r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify image dimension\n",
    "IMG_WIDTH=300\n",
    "IMG_HEIGHT=300\n",
    "IMG_DIM = (IMG_WIDTH, IMG_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules \n",
    "from io import BytesIO\n",
    "import base64\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# list of images as np.arrays \n",
    "hash_images_lst = []\n",
    "for i in range(len(hash_results)):\n",
    "    # load image \n",
    "    byteImg = Image.open(f\"{i}_\"+hash_results[i]['shortcode']+\".png\")\n",
    "    # load image \n",
    "    train_imgs2 = img_to_array(load_img(f\"{i}_\"+hash_results[i]['shortcode']+\".png\", target_size=IMG_DIM))\n",
    "    train_imgs2 = np.array(train_imgs2)\n",
    "    hash_images_lst.append(train_imgs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all images as numpy array (for feeding as X_test)\n",
    "hash_images_np = np.stack(hash_images_lst, axis=0)\n",
    "print(hash_images_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory \n",
    "path = set_root_path_images()\n",
    "\n",
    "np.save(f'{hashtag}_unofficial_npimgs.npy', hash_images_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Private API (infinite scroll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.tools'; 'tensorflow.python' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0107cebf2d34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# to install\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;31m# We still need all the names that are toplevel on tensorflow_core\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_core\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;31m# These should not be visible in the main tf module.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.tools'; 'tensorflow.python' is not a package"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import choice\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "# load modules \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import shutil \n",
    "# to install\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pip install git+https://git@github.com/ping/instagram_private_api.git@1.6.0\n",
    "from instagram_private_api import Client, ClientCompatPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = 'chenpeling@hotmail.com'\n",
    "password = 'Instagram2020'\n",
    "\n",
    "USERNAME = 'nestle' # official nestle page: 'nestle'\n",
    "HASHTAG = 'longines'\n",
    "LIMIT_IMAGE_COUNT = 72 # 1st: 0, 2nd: 36, 3rd: 72 stops, at the first step over 50 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup Instagram Private API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize client \n",
    "api = Client(user_name, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get URLs of Image Posts on Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 333 image post urls were retrieved from the Instagram page.\n"
     ]
    }
   ],
   "source": [
    "# all images urls \n",
    "all_hash_image_posts_urls = []\n",
    "\n",
    "next_max_id = None\n",
    "while (api.feed_tag(HASHTAG, api.generate_uuid())[\"more_available\"] == True) and (len([item for sublist in all_hash_image_posts_urls for item in sublist]) <= LIMIT_IMAGE_COUNT): \n",
    "    if next_max_id == None: \n",
    "        #Gets the first 12 posts\n",
    "        posts = api.feed_tag(HASHTAG, api.generate_uuid())\n",
    "        len(posts['items'])\n",
    "        image_urls = []\n",
    "        for i in range(len(posts['items'])): \n",
    "            try: \n",
    "                url = posts['items'][i]['image_versions2']['candidates'][0]['url'] # some posts do not have 'image_version2', they are overlooked in that case\n",
    "                image_urls.append(url)\n",
    "            except: \n",
    "                pass \n",
    "        # Extract the value *next_max_id* from the above response, this is needed to load the next 12 posts\n",
    "        next_max_id = posts[\"next_max_id\"] \n",
    "        all_hash_image_posts_urls.append(image_urls)\n",
    "    else: \n",
    "        next_page_posts = api.feed_tag(HASHTAG, api.generate_uuid())\n",
    "        len(next_page_posts['items'])\n",
    "        # get image urls \n",
    "        next_image_urls = []\n",
    "        for i in range(len(next_page_posts['items'])):\n",
    "            try: \n",
    "                url = next_page_posts['items'][i]['image_versions2']['candidates'][0]['url']\n",
    "                next_image_urls.append(url)\n",
    "            except: \n",
    "                pass\n",
    "        # Extract the value *next_max_id*\n",
    "        next_max_id = next_page_posts[\"next_max_id\"] \n",
    "        all_hash_image_posts_urls.append(next_image_urls)\n",
    "\n",
    "else:        \n",
    "    flat_hash_image_posts_urls = [item for sublist in all_hash_image_posts_urls for item in sublist]\n",
    "    print(f\"A total of {len(flat_hash_image_posts_urls)} image post urls were retrieved from the Instagram page.\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all images urls \n",
    "all_image_posts_urls = []\n",
    "\n",
    "next_max_id = None\n",
    "while (api.username_feed(USERNAME, max_id = next_max_id)[\"more_available\"] == True)  and (len([item for sublist in all_image_posts_urls for item in sublist]) <= LIMIT_IMAGE_COUNT): \n",
    "    if next_max_id == None: \n",
    "        #Gets the first 12 posts\n",
    "        posts = api.username_feed(USERNAME)\n",
    "        len(posts['items'])\n",
    "        image_urls = []\n",
    "        for i in range(len(posts['items'])): \n",
    "            url = posts['items'][i]['image_versions2']['candidates'][0]['url']\n",
    "            image_urls.append(url)\n",
    "        # Extract the value *next_max_id* from the above response, this is needed to load the next 12 posts\n",
    "        next_max_id = posts[\"next_max_id\"] \n",
    "        all_image_posts_urls.append(image_urls)\n",
    "    else: \n",
    "        next_page_posts = api.username_feed(USERNAME, max_id = next_max_id)\n",
    "        len(next_page_posts['items'])\n",
    "        # get image urls \n",
    "        next_image_urls = []\n",
    "        for i in range(len(next_page_posts['items'])):\n",
    "            try: \n",
    "                url = next_page_posts['items'][i]['image_versions2']['candidates'][0]['url']\n",
    "                next_image_urls.append(url)\n",
    "            except: \n",
    "                pass\n",
    "        # Extract the value *next_max_id*\n",
    "        next_max_id = next_page_posts[\"next_max_id\"] \n",
    "        all_image_posts_urls.append(next_image_urls)\n",
    "        \n",
    "else: \n",
    "    flat_image_posts_urls = [item for sublist in all_image_posts_urls for item in sublist]\n",
    "    print(f\"A total of {len(flat_image_posts_urls)} image post urls were retrieved from the Instagram page.\")\n",
    "\n",
    "\n",
    "\n",
    "### Request Errors: \n",
    "# ClientConnectionError: URLError <urlopen error timed out>\n",
    "        \n",
    "# if Timeout: spyder/plugins/ipythonconsole/comms/kernelcomm.py \n",
    "# timeout = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save Images to PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\lsamsi\\\\Documents\\\\GitHub\\\\DLfM_BrandManagement\\\\data\\\\instagram_images\\\\football'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-42b01678f52f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# set directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\lsamsi\\Documents\\GitHub\\DLfM_BrandManagement\\data\\instagram_images\\football\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\lsamsi\\\\Documents\\\\GitHub\\\\DLfM_BrandManagement\\\\data\\\\instagram_images\\\\football'"
     ]
    }
   ],
   "source": [
    "# set directory\n",
    "directory= r\"C:\\Users\\lsamsi\\Documents\\GitHub\\DLfM_BrandManagement\\data\\instagram_images\\football\"\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save HASHTAG images to PC \n",
    "for i in range(len(flat_hash_image_posts_urls)):\n",
    "    r = requests.get(flat_hash_image_posts_urls[i], stream=True)\n",
    "    with open(f\"{i}_\"+flat_hash_image_posts_urls[i][-34:]+\".png\", 'wb') as f:\n",
    "        # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "        r.raw.decode_content = True\n",
    "        # Copy the response stream raw data to local image file.\n",
    "        shutil.copyfileobj(r.raw, f)\n",
    "        # Remove the image url response object.\n",
    "        del r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save USERNAME images to PC \n",
    "for i in range(len(flat_image_posts_urls)):\n",
    "    for j in range(len(flat_image_posts_urls[i])):\n",
    "        r = requests.get(flat_image_posts_urls[i][j], stream=True)\n",
    "        with open(f\"{i}_\"+flat_image_posts_urls[i][j][-34:]+\".png\", 'wb') as f:\n",
    "            # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "            r.raw.decode_content = True\n",
    "            # Copy the response stream raw data to local image file.\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "            # Remove the image url response object.\n",
    "            del r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR VINCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://git@github.com/ping/instagram_private_api.git@1.6.0\n",
    "from instagram_private_api import Client, ClientCompatPatch\n",
    "\n",
    "USERNAME = 'nestle' # official nestle page: 'nestle'\n",
    "HASHTAG = 'longines'\n",
    "LIMIT_IMAGE_COUNT = 50 # 1st: 0, 2nd: 36, 3rd: 72 stops, at the first step over 50 \n",
    "\n",
    "\n",
    "def official_images_unlimited(USERNAME, LIMIT_IMAGE_COUNT): \n",
    "\n",
    "    user_name = 'chenpeling@hotmail.com'\n",
    "    password = 'Instagram2020'\n",
    "\n",
    "    # initialize client \n",
    "    api = Client(user_name, password)\n",
    "\n",
    "    ########### USERNAME ###################\n",
    "\n",
    "    # all images urls \n",
    "    all_image_posts_urls = []\n",
    "\n",
    "    next_max_id = None\n",
    "    while (api.username_feed(USERNAME, max_id = next_max_id)[\"more_available\"] == True)  and (len([item for sublist in all_image_posts_urls for item in sublist]) <= LIMIT_IMAGE_COUNT): \n",
    "        if next_max_id == None: \n",
    "            #Gets the first 12 posts\n",
    "            posts = api.username_feed(USERNAME)\n",
    "            len(posts['items'])\n",
    "            image_urls = []\n",
    "            for i in range(len(posts['items'])): \n",
    "                url = posts['items'][i]['image_versions2']['candidates'][0]['url']\n",
    "                image_urls.append(url)\n",
    "            # Extract the value *next_max_id* from the above response, this is needed to load the next 12 posts\n",
    "            next_max_id = posts[\"next_max_id\"] \n",
    "            all_image_posts_urls.append(image_urls)\n",
    "        else: \n",
    "            next_page_posts = api.username_feed(USERNAME, max_id = next_max_id)\n",
    "            len(next_page_posts['items'])\n",
    "            # get image urls \n",
    "            next_image_urls = []\n",
    "            for i in range(len(next_page_posts['items'])):\n",
    "                try: \n",
    "                    url = next_page_posts['items'][i]['image_versions2']['candidates'][0]['url']\n",
    "                    next_image_urls.append(url)\n",
    "                except: \n",
    "                    pass\n",
    "            # Extract the value *next_max_id*\n",
    "            next_max_id = next_page_posts[\"next_max_id\"] \n",
    "            all_image_posts_urls.append(next_image_urls)\n",
    "\n",
    "    else: \n",
    "        flat_image_posts_urls = [item for sublist in all_image_posts_urls for item in sublist]\n",
    "        #print(f\"A total of {len(flat_image_posts_urls)} image post urls were retrieved from the Instagram page.\")\n",
    "    \n",
    "    return flat_image_posts_urls\n",
    "\n",
    "\n",
    "### Request Errors: \n",
    "# ClientConnectionError: URLError <urlopen error timed out>\n",
    "\n",
    "# if Timeout: spyder/plugins/ipythonconsole/comms/kernelcomm.py \n",
    "# timeout = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unofficial_images_unlimited(USERNAME, LIMIT_IMAGE_COUNT): \n",
    "\n",
    "    user_name = 'chenpeling@hotmail.com'\n",
    "    password = 'Instagram2020'\n",
    "\n",
    "    # initialize client \n",
    "    api = Client(user_name, password)\n",
    "    \n",
    "    #%%\n",
    "########### HASHTAG ###################\n",
    "\n",
    "    # all images urls \n",
    "    all_hash_image_posts_urls = []\n",
    "\n",
    "    next_max_id = None\n",
    "    while (api.feed_tag(HASHTAG, api.generate_uuid())[\"more_available\"] == True) and (len([item for sublist in all_hash_image_posts_urls for item in sublist]) <= LIMIT_IMAGE_COUNT): \n",
    "        if next_max_id == None: \n",
    "            #Gets the first 12 posts\n",
    "            posts = api.feed_tag(HASHTAG, api.generate_uuid())\n",
    "            len(posts['items'])\n",
    "            image_urls = []\n",
    "            for i in range(len(posts['items'])): \n",
    "                try: \n",
    "                    url = posts['items'][i]['image_versions2']['candidates'][0]['url'] # some posts do not have 'image_version2', they are overlooked in that case\n",
    "                    image_urls.append(url)\n",
    "                except: \n",
    "                    pass \n",
    "            # Extract the value *next_max_id* from the above response, this is needed to load the next 12 posts\n",
    "            next_max_id = posts[\"next_max_id\"] \n",
    "            all_hash_image_posts_urls.append(image_urls)\n",
    "        else: \n",
    "            next_page_posts = api.feed_tag(HASHTAG, api.generate_uuid())\n",
    "            len(next_page_posts['items'])\n",
    "            # get image urls \n",
    "            next_image_urls = []\n",
    "            for i in range(len(next_page_posts['items'])):\n",
    "                try: \n",
    "                    url = next_page_posts['items'][i]['image_versions2']['candidates'][0]['url']\n",
    "                    next_image_urls.append(url)\n",
    "                except: \n",
    "                    pass\n",
    "            # Extract the value *next_max_id*\n",
    "            next_max_id = next_page_posts[\"next_max_id\"] \n",
    "            all_hash_image_posts_urls.append(next_image_urls)\n",
    "\n",
    "    else:        \n",
    "        flat_hash_image_posts_urls = [item for sublist in all_hash_image_posts_urls for item in sublist]\n",
    "        print(f\"A total of {len(flat_hash_image_posts_urls)} image post urls were retrieved from the Instagram page.\")\n",
    "    \n",
    "    return flat_hash_image_posts_urls\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
