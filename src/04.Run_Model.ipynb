{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "To predict the presence of perceptual brand attributes in the images that consumers post online. \n",
    "\n",
    "### Please refer to notebook Train_Validation_Test_Split for more details about problem statement.\n",
    "\n",
    "This notebook expects that user has already run Train_Validation_Test_Split to preprocess the training data. \n",
    "\n",
    "Since we are using one vs rest approach to solve the multiclass problem, we need to create 4 classfiers, one for each attribute. Each classifier needs to have its own dataset, with one attribute beling classfied as positive, and all other images being classified as negative. \n",
    "\n",
    "Since training dataset is quite large, and we cannot load 4 datasets into memory at the same time, we need to run this notebook 4 times to create/train 4 classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "try: \n",
    "  print('set memory growth')\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
    "except: \n",
    "  # Invalid device or cannot modify virtual devices once initialized. \n",
    "  pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH=300\n",
    "IMG_HEIGHT=300\n",
    "IMG_DIM = (IMG_WIDTH, IMG_HEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training and validation data. \n",
    "Change the training/validation data file name corresponding to the model being trained. We need tp train 4 models, and for each model we need to use different dataset.\n",
    "\n",
    "We have already save X_train/y_train in the previous notebook. Now we read training/validation data corresponding to each attribute and train the corresponding model. \n",
    "\n",
    " - X_train_0 / X_val_0 : Training / Validation data corresponding to Attribute Glamarous\n",
    " - X_train_1 / X_val_1 : Training / Validation data corresponding to Attribute Rugged\n",
    " - X_train_2 / X_val_2 : Training / Validation data corresponding to Attribute Fun\n",
    " - X_train_3 / X_val_3 : Training / Validation data corresponding to Attribute Healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('5000//X_train_3.npy')\n",
    "X_val = np.load('5000//X_val_3.npy')\n",
    "\n",
    "y_train = np.load('5000//y_train_3.npy')\n",
    "y_val = np.load('5000//y_val_3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run this code for healthy attribute only(Disable this code while training classifiers for other attributes)\n",
    "\n",
    "For healthy attribute, number of images(400) are 1/5 of other attrivbutes(2000). \n",
    "For this attribute, training data will be higly unbalanced, as nu of positives samples is very small compared to negative samples. \n",
    "For this attrivute, we run this code, to remove negative sample from the training data so that positive and negative classes\n",
    "become balanced. \n",
    "\n",
    "'''\n",
    "\n",
    "index = np.where(y_train == 1)\n",
    "\n",
    "X_pos = X_train[index]\n",
    "y_pos= y_train[index]\n",
    "\n",
    "print(X_pos.shape)\n",
    "print(y_pos.shape)\n",
    "index = np.where(y_train == 0)\n",
    "X_neg = X_train[index]\n",
    "y_neg= y_train[index]\n",
    "X_neg = X_neg[0:X_pos.shape[0]]\n",
    "y_neg = y_neg[0:X_pos.shape[0]]\n",
    "print(X_neg.shape)\n",
    "print(y_neg.shape)\n",
    "\n",
    "X_train = np.hstack((X_pos, X_neg))\n",
    "y_train = np.hstack((y_pos, y_neg))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image data into ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= X_train.flatten()\n",
    "X_train = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in X_train.tolist()]\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val=   X_val.flatten()\n",
    "X_val = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in X_val.tolist()]\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "print( X_train.shape)\n",
    "print( y_train.shape)\n",
    "print( X_val.shape)\n",
    "print( y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data augmentation framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = datagen.flow(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    " - We are going to use pre trained resnet model \n",
    " - do not want to load the last fully connected layers which act as the classifier \n",
    " - Freeze the weights of the model by setting trainable as false\n",
    " - add our own fully connected layers on top of the ResNet50 model for our task-specific classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restnet = ResNet50(include_top=False, weights='imagenet', input_shape=(IMG_HEIGHT,IMG_WIDTH,3))\n",
    "# We do not want to load the last fully connected layers which act as the classifier we can add our \n",
    "# own fully connected layers on top of the ResNet50 model for our task-specific classification.\n",
    "\n",
    "output = restnet.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "restnet = Model(restnet.input, output=output)\n",
    "\n",
    "#freeze the weights of the model by setting trainable as “False”\n",
    "restnet.trainable = True\n",
    "set_trainable = False\n",
    "    \n",
    "for layer in restnet.layers:\n",
    "    if layer.name in ['res5c_branch2b', 'res5c_branch2c', 'activation_97']:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "restnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add task specific fully connected layer to fine tune the model for classification purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(IMG_HEIGHT,IMG_WIDTH,3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(restnet)\n",
    "model.add(Dense(64, activation='relu', input_dim=input_shape))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history =      model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        epochs=4,\n",
    "                        steps_per_epoch=len(X_train) / 32,\n",
    "                        validation_data=(X_val,y_val) ,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history =      model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=3,\n",
    "                        batch_size = 30,\n",
    "                        validation_data=(X_val,y_val) ,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "We need to save 4 models. As we cannot train the model simultaneously because of gpu memory limitations, we need to run this notebook 4 times, to train each model independently. Please change the model name corresponding to the attribute being trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./savemodels_4/healthy_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/my_checkpoint')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
