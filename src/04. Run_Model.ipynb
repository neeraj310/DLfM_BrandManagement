{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "To predict the presence of perceptual brand attributes in the images that consumers post online. \n",
    "\n",
    "### Please refer to notebook Train_Validation_Test_Split for more details about problem statement.\n",
    "\n",
    "This notebook expects that user has already run Train_Validation_Test_Split to preprocess the training data. \n",
    "\n",
    "Since we are using one vs rest approach to solve the multiclass problem, we need to create 4 classfiers, one for each attribute. Each classifier needs to have its own dataset, with one attribute beling classfied as positive, and all other images being classified as negative. \n",
    "\n",
    "Since training dataset is quite large, and we cannot load 4 datasets into memory at the same time, we need to run this notebook 4 times to create/train 4 classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "try: \n",
    "  print('set memory growth')\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
    "except: \n",
    "  # Invalid device or cannot modify virtual devices once initialized. \n",
    "  pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH=300\n",
    "IMG_HEIGHT=300\n",
    "IMG_DIM = (IMG_WIDTH, IMG_HEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training and validation data. \n",
    "Change the training/validation data file name corresponding to the model being trained. We need tp train 4 models, and for each model we need to use different dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('5000//X_train_0.npy')\n",
    "X_val = np.load('5000//X_val_0.npy')\n",
    "\n",
    "y_train = np.load('5000//y_train_0.npy')\n",
    "y_val = np.load('5000//y_val_0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the image into ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= X_train.flatten()\n",
    "X_train = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in X_train.tolist()]\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val=   X_val.flatten()\n",
    "X_val = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in X_val.tolist()]\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "y_val = np.array(y_val)\n",
    "print( X_train.shape)\n",
    "print( y_train.shape)\n",
    "print( X_val.shape)\n",
    "print( y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    " - We are going to use pre trained resnet model \n",
    " - do not want to load the last fully connected layers which act as the classifier \n",
    " - Freeze the weights of the model by setting trainable as false\n",
    " - add our own fully connected layers on top of the ResNet50 model for our task-specific classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restnet = ResNet50(include_top=False, weights='imagenet', input_shape=(IMG_HEIGHT,IMG_WIDTH,3))\n",
    "# We do not want to load the last fully connected layers which act as the classifier we can add our \n",
    "# own fully connected layers on top of the ResNet50 model for our task-specific classification.\n",
    "\n",
    "output = restnet.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "restnet = Model(restnet.input, output=output)\n",
    "\n",
    "#freeze the weights of the model by setting trainable as “False”\n",
    "restnet.trainable = True\n",
    "set_trainable = False\n",
    "    \n",
    "for layer in restnet.layers:\n",
    "    if layer.name in ['res5c_branch2b', 'res5c_branch2c', 'activation_97']:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "restnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add task specific fully connected layer to fine tune the model for classification purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_shape=(IMG_HEIGHT,IMG_WIDTH,3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(restnet)\n",
    "model.add(Dense(64, activation='relu', input_dim=input_shape))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history =      model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=1,\n",
    "                        batch_size = 30,\n",
    "                        validation_data=(X_val,y_val) ,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "We need to save 4 models. As we cannot train the model simultaneously because of gpu memory limitations, we need to run this notebook 4 times, to train each model independently. Please change the model name corresponding to the attribute being trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./savemodels/healthy_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
